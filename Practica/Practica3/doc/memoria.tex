%!TeX spellcheck = es-ES
\chapter{Ejercicio sobre la complejidad de H y el ruido}
\section{Nube de puntos}
Se muestran aquí las nubes de datos solicitadas en el ejercicio 1 habiendo utilizado las funciones:
\begin{itemize}
   \item $ simula\_unif (N, dim, rango) $, que calcula una lista de N vectores de dimensión dim. Cada
vector contiene dim números aleatorios uniformes en el intervalo rango.
   \item $ simula\_gaus(N, dim, sigma) $, que calcula una lista de longitud N de vectores de dimensión dim, donde cada posición del vector contiene un número aleatorio extraído de una distribución Gaussiana de media 0 y varianza dada, para cada dimensión, por la posición del vector sigma.
\end{itemize}


\begin{figure}[h]
   \centering
   \begin{subfigure}[b]{0.5\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_1.png}
      \caption{N = 50, dim = 2, rango = [−50, +50]}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.5\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_2.png}
      \caption{N = 50, dim = 2 y sigma = [5, 7]}
   \end{subfigure}
\end{figure}

\newpage
\section{Clasificación de puntos}
Ahora nos piden clasificar una muestra de 100 puntos utilizando las función $ simula\_unif (100, 2, [−50, 50]) $ y etiquetar los mismos utilizando la función: $ f(x,y) = y - ax -b $\\
Para obtener los valores de a y b, utilizamos la función:
\begin{itemize}
   \item $ simula\_recta(intervalo) $ , que simula de forma aleatoria los parámetros, v = (a, b) de una recta, \\y = ax + b, que corta al cuadrado [−50, 50] × [−50, 50].
   \item Valor de a: 5.0444575367390385
   \item Valor de b: -122.8600444168694
\end{itemize}

\begin{figure}[h]
   \centering
   \includegraphics[width=0.7\textwidth]{Figure_3.png}
   \caption{Recta y = ax+b}
\end{figure}

Vemos que los datos están perfectamente clasificados respecto a la recta.\\
Ahora, simplemente seleccionamos un 10\% de los datos positivos y un 10\% de los datos negativos y los multiplicamos por -1 para así introducir ruido.
\begin{figure}[h]
   \centering
   \includegraphics[width=0.65\textwidth]{Figure_4.png}
   \caption{Añadimos ruido a la clasificación}
\end{figure}

\newpage
\subsection{Prueba de diferentes funciones frontera}
Utilizaremos las siguientes funciones frontera:
\begin{itemize}
   \item $ f (x, y) = (x − 10)^{2} + (y − 20)^{2} − 400 $
   \item $ f (x, y) = 0,5(x + 10)^{2} + (y − 20)^{2} − 400 $
   \item $ f (x, y) = 0,5(x − 10)^{2} − (y + 20)^{2} − 400 $
   \item $ f (x, y) = y − 20x^{2} − 5x + 3 $
\end{itemize}

En estas gráficas vemos el mismo conjunto de datos que en el apartado anterior. El problema es que cuando existe ruido en la muestra, no podemos alcanzar un ajuste perfecto, puesto que siempre van a existir puntos que se encuentren fuera del ajuste que hemos realizado. A no ser que sobreajustemos ŵ, cosa que también nos daría bastantes problemas. En este caso el ruido se ha introducido de forma aleatoria, por tanto, funciones como la d), que dividen el espacio en una sección muy pequeña, ven que su ajuste es bastante malo, sin embargo, c), si que consigue un mejor ajuste ya que divide el espacio en áreas más grandes. Los ajustes a) y b) se encuentran en un caso intermedio, siendo a) el ajuste mas perjudicado por el ruido.

\begin{figure}[h]
   \centering
   \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_5.png}
      \caption{$ f (x, y) = (x − 10)^{2} + (y − 20)^{2} − 400 $}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_6.png}
      \caption{$ f (x, y) = 0,5(x + 10)^{2} + (y − 20)^{2} − 400 $}
   \end{subfigure}
   \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_7.png}
      \caption{$ f (x, y) = 0,5(x − 10)^{2} − (y + 20)^{2} − 400 $}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_8.png}
      \caption{$ f (x, y) = y − 20x^{2} − 5x + 3 $}
   \end{subfigure}
\end{figure}

\chapter{Modelos Lineales}
\section{Algoritmo Perceptron}
Aquí podemos ver la implementación de la función $ ajusta\_PLA(datos, label, max_iter, vini) $:
\begin{minted}
[fontsize=\footnotesize, linenos]
{python}
def ajusta_PLA(datos, label, max_iter, vini):
   # datos: conjunto de entrenamiento D
   # label: conjunto de etiquetas asociado a D
   # max_iter: numero máximo de iteraciones
   # vini: valor inicial para w
   # Establecemos vini como valor inicial de w
   w = np.copy(vini)
   mejora = True
   iter = 0
   # Si ha habido mejora y las iteraciones no superan un maximo
   while(mejora and iter < max_iter):
      mejora = False
      iter = iter + 1
      # Para cada xi del conjunto D
      for i in range(len(datos)):
         if(signo(w.T.dot(datos[i])) != label[i]):
            # w_new = w_old + xi*yi
            w = w + label[i]*datos[i]
            mejora = True

   # Devuelve el valor de w y las iteraciones utilizadas
   return w, iter
\end{minted}

El Perceptron Learning Algorithm (PLA) se basa en recorrer el conjunto de entrenamiento D, ajustando ŵ en cada iteración en base a un solo elemento del conjunto, sin prestar atención a los demás.
Sabemos que aun así, este algoritmo alcanza un óptimo en tiempo finito.  Además, para un conjunto de datos linealmente separable, PLA siempre es capaz de encontrar el vector de pesos ŵ, tal que $ h(x_{i}) = y_{i} $.

\begin{figure}[h]
   \centering
   \includegraphics[width=0.65\textwidth]{Figure_9.png}
   \caption{Algoritmo PLA}
\end{figure}

Como vemos en la Figura 2.1, el algoritmo PLA es capaz de encontrar un valor para ŵ que separa casi a la perfección los datos del conjunto.\\
En la figura vemos la recta que obtenemos de la función ajusta\_PLA para una valor de vini = \{0,0,0\}.\\
Para obtener el valor de ŵ, el algoritmo ha utilizado un total de 43 iteraciones.\\
Probando con 10 vectores de datos aleatorios en el intervalo [0,1] la media de iteraciones obtenida es:\\ \textbf{48.45 iteraciones}
\\\\
El problema del algoritmo PLA, viene cuando introducimos ruido en el conjunto de datos.
Al introducir ruido, ya no podemos separar linealmente los datos, por lo que el algoritmo PLA itera de manera infinita, ajustando el vector ŵ a cada uno de los puntos.
Como estos puntos no se pueden separar linealmente, el algoritmo PLA nunca termina. Por ello, si introducimos ruido en el conjunto de datos, el número de iteraciones utilizadas por el algoritmo PLA es siempre el número máximo de iteraciones.
\\
Aquí vemos el resultado del ajuste después de 1000 y 10000 iteraciones.

\begin{figure}[h]
   \centering
   \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_10.png}
      \caption{1000 iteraciones de PLA}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_11.png}
      \caption{10000 iteraciones de PLA}
   \end{subfigure}
\end{figure}

Vemos que la recta es distinta dependiendo del número de iteraciones, porque como en cada iteración se ajusta ŵ en función de un dato $ x_{i} \in D $, el ultimo dato que ajusta ŵ es el que determina el valor obtenido para este.

\newpage
\section{Regresión Logística}
Implementación de la Regresión Logística (RL) con Gradiente Descendente Estocástico (SGD):
\begin{minted}
[fontsize=\footnotesize, linenos]
{python}
def E(x,y,w):

   return (-(y*x) / (1 + np.exp(y*np.dot(w.T,x))))

def sgdRL(x,y,n):
   # x: conjunto de datos
   # y: conjunto de etiquetas
   # n: tasa de aprendizaje
   # Inicializamos w a {0,0,0}
   w = np.zeros(x[0].size)
   # Guardamos una copia de w en w_ant
   w_ant = np.copy(w)
   # Obtenemos los indices en orden aleatorio
   batch = np.random.choice(np.size(x,0), np.size(x,0), replace=False)
   # Obtenemos el valor de w(0)
   # Para cada indice
   for j in batch:
      w = w - n * E(x[j],y[j],w)
   #Mientras ||w_ant - w|| >= 0.01, seguimos calculando w(t)
   while np.linalg.norm(w_ant - w) >= 0.01:
      # Guardamos una copia de w en w_ant
      w_ant = np.copy(w)
      # Obtenemos los indices en orden aleatorio
      batch = np.random.choice(np.size(x,0), np.size(x,0), replace=False)
      # Para cada indice
      for j in batch:
         w = w - n * E(x[j],y[j],w)

   return w
\end{minted}
En la implementación del SGD, utilizamos minibatches de tamaño 1.\\
Podemos ver que el algortimo se basa en aplicar el gradiente:\\
$ \nabla E_{in} = \frac{ y_{n} x_{n} }{ 1 + e^{y_{n}w^{T}(t)x_{n} } } $\\
E ir actualizando el valor de ŵ hasta que se cumpla la condición : $ \left \| w^{t+1} - w^{t} \right \| \geq 0.01 $

\begin{figure}[h]
   \centering
   \includegraphics[width=0.7\textwidth]{Figure_12.png}
   \caption{}
\end{figure}

En la Figura 2.3 se puede apreciar que el ajuste para ŵ que aporta el algoritmo es muy bueno, puesto que casi se superpone con la recta de clasificación.\\
\\
Utilizando este ajuste, se ha generado un nuevo conjunto de 1000 puntos, y se ha obtenido una estimación de $ \mathbf{E_{out} = 0.09834679095021306} $, lo que es un error bastante bajo.



\chapter{Bonus}

\section{Problema de clasificación}
El problema consiste en clasificar una serie de números escritos a mano. Utilizaremos los el data set de números usado en la practica anterior, que contiene los valores de intensidad promedio y simetría de diferentes números escritos a mano. En este caso clasificaremos los números 4 y 8.\\
Intentaremos encontrar la función $ \mathbf{h(x) = w^{T}x} $ que sea capaz de clasificar estos datos.
Para ello, utilizaremos la función de regresión lineal del Gradiente Descendente Estocástico (SGD) a la que después aplicaremos la extensión del algoritmo PLA conocida como pocket.

\begin{figure}[h]
   \centering
   \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_13.png}
      \caption{Conjunto de entrenamiento}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_14.png}
      \caption{Conjunto de prueba}
   \end{subfigure}
\end{figure}

\section{Calculo del $ E_{in} $ y $ E_{test} $}

Implementación del SGD:
\begin{minted}
[fontsize=\footnotesize, linenos]
{python}
def sgd(x,y,n,iterations):
   w = np.zeros(x[0].size) # Inicializamos w al vector de tantos 0's como
   # caracteristicas tiene x
   c = 0
   epoca = 0
   # Mientras no se supere el numero maximo de iteraciones y no hayamos terminado una epoca
   termianda_epoca = True
   batch = np.array([])
   while c < iterations or not termianda_epoca:
      # Obtenemos la submuestra de X
      if len(batch) == 0:
         termianda_epoca = False
         batch = np.random.choice(np.size(x,0), np.size(x,0), replace=False)

      minibatch = []
      index = []
      for i in range(32):
         if i == len(batch):
            break
         minibatch.append(batch[i])
         index.append(i)

      batch = np.delete(batch,index)

      # Copiamos en w_ant el valor anterior de w
      w_ant = np.copy(w)
      # Para cada wj
      for i in range(np.size(w)):
         sumatoria = 0
         # Calculamos la sumatoria de cada x que pertenece a la submuestra
         for j in minibatch:
            c = c + 1
            sumatoria = sumatoria + x[j][i]*(np.dot(x[j],w.T) - y[j])
            # Actualizamos el valor de wj
            w[i] = w_ant[i] -n * (2.0/np.float(np.size(minibatch))) * sumatoria

      if len(batch) == 0:
         termianda_epoca = True
         epoca +=1
      # Devolvemos w
   return w

\end{minted}
Aplicamos un ajuste utilizando SGD con un limite de 10000 iteraciones.\\
En las siguientes figuras, vemos el ajuste que ofrece el algoritmo SGD, que en principio, no parece bastante bueno.
\begin{figure}[h]
   \centering
   \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_15.png}
      \caption{Conjunto de entrenamiento}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_16.png}
      \caption{Conjunto de prueba}
   \end{subfigure}
\end{figure}

Con este ajuste obtenemos unos errores de:
\begin{itemize}
   \item $  \mathbf{E_{in} = 0.40117252931323283} $
   \item $  \mathbf{E_{test} = 0.40437158469945356} $
\end{itemize}

Implementación del algoritmo pocket:
\begin{minted}
[fontsize=\footnotesize, linenos]
{python}
def pocket_PLA(datos, label, w):
   # datos: conjunto de entrenamiento D
   # label: conjunto de etiquetas asociado a D
   # w: valor inicial del vector w
   # Establecemos como Error minimo, el valor de Error(w)
   err_min = Err(datos,label,w)
   w_min = w
   # Hacemos 100 ejecuciones del PLA con 1 iteración
   for i in range(100):
      vini = np.copy(w)
      w = ajusta_PLA(datos, label, 1, vini)
      err_w = Err(datos,label,w)
      # Nos quedamos con el w que produce un error minimo
      if err_w < err_min:
         w_min = w
         err_min = err_w

   # Devuelve el w que produce el error minimo
   return w_min
\end{minted}

El algortimo pocket es una extensión del algoritmo PLA que se basa en ``guardar en el bolsillo'' el mejor valor obtenido para ŵ.
De esta forma, se puede utilizar el ajuste que realiza el algortimo PLA a un conjunto que no sea totalmente separable por una recta.
Pocket, ejecuta de manera reiterada el algortimo PLA y se queda siempre con el mejor ajuste que este proporciona.


\begin{figure}[h]
   \centering
   \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_17.png}
      \caption{Conjunto de entrenamiento}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=1.15\textwidth]{Figure_18.png}
      \caption{Conjunto de prueba}
   \end{subfigure}
\end{figure}

Para establecer el error necesario para poder aplicar el algoritmo pocket, se utiliza la siguiente función de error:

\begin{minted}
[fontsize=\footnotesize, linenos]
{python}
def Err(x,y,w):
   # x: conjunto de datos
   # y: conjunto de etiquetas
   # w: valores de ŵ
   # Contamos los puntos que estan mal clasificados
   cont = 0
   for i in range(len(x)):
      if signo(np.dot(w.T,x[i])) != y[i]:
         cont += 1

   return cont / len(x)
\end{minted}

Esta función simplemente cuenta el porcentaje de puntos mal clasificados respecto al total.\\
Una vez hemos aplicado el algoritmo pocket, visualmente el ajuste parece bastante bueno y obtenemos unos errores de:
\begin{itemize}
   \item $ \mathbf{E_{in} = 0.2269681742043551} $
   \item $  \mathbf{E_{test} = 0.25956284153005466} $
\end{itemize}

\section{Cota de $ E_{out} $}
Podemos establecer dos cotas para $ E_{out} $, utilizando el $ E_{in} $ y utilizando $ E_{test} $
\begin{itemize}
   \item Cota obtenida con $ E_{in} $: $ \mathbf{E_{out} = 0.28255156903289125} $
   \item Cota obtenida con $ E_{test} $: $ \mathbf{E_{out} = 0.3151462363585908} $
\end{itemize}

La mejor cota es la obtenida utilizando el $ E_{in} $.\\
Para establecer la cota se ha utilizado: $ E_{out} \leq E_{in} + \sqrt{\frac{1}{2N}log(\frac{2}{\delta})}s $

\chapter{Bibliografía}
\begin{itemize}

   \item \href{https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html}{Docuemntación de numpy.array}
   \item \href{https://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html}{Docuemntación de numpy.meshgrid}
   \item \href{https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html}{Docuemntación de numpy.random.choice}
   \item \href{https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html}{Docuemntación de numpy.linalg.inv}
   \item \href{https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html}{Docuemntación de numpy.dot}
   \item \href{https://het.as.utexas.edu/HET/Software/Numpy/reference/generated/numpy.mean.html#numpy.mean}{Docuemntación de numpy.mean}
   \item \href{https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html#numpy.zeros}{Docuemntación de numpy.zeros}
   \item \href{https://stackoverflow.com/questions/32092899/plot-equation-showing-a-circle}{Cómo pintar un circulo con matplotlib}
   \item \href{https://matplotlib.org/}{Documentación de matplotlib}
   \item \href{https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html}{Docuemntación de numpy.linalg.norm}
   \item \href{https://docs.scipy.org/doc/numpy/reference/generated/numpy.delete.html}{Docuemntación de numpy.delete}

\end{itemize}
